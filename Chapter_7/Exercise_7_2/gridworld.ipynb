{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.seed(7)\n",
    "import matplotlib.pyplot as plt\n",
    "from gridworld import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_STEPS_PER_EPISODE = 10000\n",
    "environment = Environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        , -14.17754254, -20.01514716, -22.0802292 ,\n",
       "       -14.00443962, -18.05055685, -20.08190474, -20.09216177,\n",
       "       -19.98935926, -20.07798407, -18.01347044, -13.97455873,\n",
       "       -22.08353588, -20.11964308, -13.86883972])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TD(0)\n",
    "num_episodes = 100000\n",
    "step_size = 0.001\n",
    "V = np.zeros(len(environment.transitions))\n",
    "for ep in range(num_episodes):\n",
    "    state = environment.generate_random_start_state()\n",
    "    for t in range(MAX_NUM_STEPS_PER_EPISODE):\n",
    "        action = random.choice(environment.available_actions)\n",
    "        reward, new_state, reached_terminal_state = environment.take_action(state, action)\n",
    "        V[state] += step_size * (reward + V[new_state] - V[state])\n",
    "        state = new_state\n",
    "        if reached_terminal_state:\n",
    "            break\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        , -14.17636992, -20.2604835 , -22.08727368,\n",
       "       -14.39232653, -18.06449959, -20.03788821, -19.74621428,\n",
       "       -20.40110388, -19.71846281, -17.73341203, -13.77868259,\n",
       "       -21.91738656, -19.92979087, -13.70439255])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n-step TD\n",
    "num_episodes = 100000\n",
    "step_size = 0.001\n",
    "n = 3\n",
    "V = np.zeros(len(environment.transitions))\n",
    "for ep in range(num_episodes):\n",
    "    rewards = []\n",
    "    states = [environment.generate_random_start_state()]\n",
    "    T = np.inf\n",
    "    for t in range(MAX_NUM_STEPS_PER_EPISODE):\n",
    "        if t < T:\n",
    "            action = random.choice(environment.available_actions)\n",
    "            reward, new_state, reached_terminal_state = environment.take_action(states[t], action)\n",
    "            rewards.append(reward)\n",
    "            states.append(new_state)\n",
    "            if reached_terminal_state:\n",
    "                T = t + 1\n",
    "        tau = t - n + 1\n",
    "        if tau >= 0:\n",
    "            G = np.sum(rewards[tau:min(tau + n, T)])\n",
    "            if tau + n < T:\n",
    "                G = G + V[states[tau + n]]\n",
    "            V[states[tau]] += step_size * (G - V[states[tau]])\n",
    "        if tau == T - 1:\n",
    "            break\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        , -14.35510568, -20.13663556, -22.19066536,\n",
       "       -14.02919641, -17.99393937, -20.22182083, -20.15391935,\n",
       "       -19.97488105, -20.06673055, -17.97408032, -14.18305996,\n",
       "       -22.21563514, -20.23225319, -13.77969526])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n-step sum of TD errors\n",
    "num_episodes = 100000\n",
    "step_size = 0.001\n",
    "n = 3\n",
    "V = np.zeros(len(environment.transitions))\n",
    "for ep in range(num_episodes):\n",
    "    td_errors = []\n",
    "    states = [environment.generate_random_start_state()]\n",
    "    T = np.inf\n",
    "    for t in range(MAX_NUM_STEPS_PER_EPISODE):\n",
    "        if t < T:\n",
    "            action = random.choice(environment.available_actions)\n",
    "            reward, new_state, reached_terminal_state = environment.take_action(states[t], action)\n",
    "            td_errors.append(reward + V[new_state] - V[states[t]])\n",
    "            states.append(new_state)\n",
    "            if reached_terminal_state:\n",
    "                T = t + 1\n",
    "        tau = t - n + 1\n",
    "        if tau >= 0:\n",
    "            V[states[tau]] += step_size * np.sum(td_errors[tau:min(tau + n, T)])\n",
    "        if tau == T - 1:\n",
    "            break\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO: do multiple runs per method befor plotting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f26179ee78db2aa81e6834ad6dc21cf32f3392ab3632a5346cf30ea04832c43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
